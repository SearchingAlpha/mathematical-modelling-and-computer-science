{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Exercise 1.4: Taxonomies of Complexity\n",
       "\n",
       "In this notebook, we'll explore different ways to classify and measure complexity. These taxonomies help us understand the various dimensions of complex systems and provide frameworks for comparing systems across different domains."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Import necessary libraries\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import pandas as pd\n",
       "import networkx as nx\n",
       "from scipy import stats\n",
       "from sklearn.cluster import KMeans\n",
       "import seaborn as sns\n",
       "from IPython.display import HTML\n",
       "from itertools import product\n",
       "import warnings\n",
       "warnings.filterwarnings('ignore')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Major Taxonomies of Complexity\n",
       "\n",
       "Several frameworks exist for classifying complexity. We'll implement metrics for some of the most important ones:\n",
       "\n",
       "1. **Structural Complexity**: Complexity in the system's organization or architecture\n",
       "2. **Dynamic Complexity**: Complexity in the system's behavior over time\n",
       "3. **Computational Complexity**: Difficulty in simulating or predicting the system\n",
       "4. **Informational Complexity**: Information content or processing in the system"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Structural Complexity Measures\n",
       "\n",
       "### 1.1 Network-Based Complexity\n",
       "\n",
       "Networks provide a powerful framework for measuring structural complexity. Let's create some example networks with different complexity levels:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def calculate_network_metrics(G):\n",
       "    \"\"\"Calculate various complexity metrics for a network.\"\"\"\n",
       "    metrics = {}\n",
       "    \n",
       "    # Basic metrics\n",
       "    metrics['nodes'] = G.number_of_nodes()\n",
       "    metrics['edges'] = G.number_of_edges()\n",
       "    metrics['avg_degree'] = np.mean([d for n, d in G.degree()])\n",
       "    \n",
       "    # Connectedness\n",
       "    try:\n",
       "        metrics['diameter'] = nx.diameter(G)\n",
       "    except:\n",
       "        metrics['diameter'] = np.nan  # Not connected\n",
       "    \n",
       "    # Clustering\n",
       "    metrics['clustering'] = nx.average_clustering(G)\n",
       "    \n",
       "    # Component analysis\n",
       "    components = list(nx.connected_components(G))\n",
       "    metrics['num_components'] = len(components)\n",
       "    \n",
       "    # Centrality measures\n",
       "    centrality = nx.degree_centrality(G)\n",
       "    metrics['centrality_std'] = np.std(list(centrality.values()))\n",
       "    \n",
       "    # Modularity using community detection\n",
       "    if metrics['nodes'] > 2:  # Need enough nodes\n",
       "        try:\n",
       "            communities = nx.algorithms.community.greedy_modularity_communities(G)\n",
       "            metrics['num_communities'] = len(communities)\n",
       "        except:\n",
       "            metrics['num_communities'] = 1\n",
       "    else:\n",
       "        metrics['num_communities'] = 1\n",
       "        \n",
       "    # Calculate network complexity index (composite metric)\n",
       "    metrics['complexity_index'] = (metrics['clustering'] * \n",
       "                                  np.log(1 + metrics['avg_degree']) * \n",
       "                                  metrics['centrality_std'] * \n",
       "                                  np.log(1 + metrics['num_communities']))\n",
       "    \n",
       "    return metrics"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create networks with different structures\n",
       "networks = {\n",
       "    'Random': nx.erdos_renyi_graph(30, 0.1),\n",
       "    'Small-World': nx.watts_strogatz_graph(30, 4, 0.2),\n",
       "    'Scale-Free': nx.barabasi_albert_graph(30, 2),\n",
       "    'Hierarchical': nx.balanced_tree(2, 4),  # Binary tree with depth 4\n",
       "    'Regular': nx.random_regular_graph(4, 30)  # Each node has exactly 4 connections\n",
       "}\n",
       "\n",
       "# Calculate metrics for each network\n",
       "network_results = {}\n",
       "for name, G in networks.items():\n",
       "    network_results[name] = calculate_network_metrics(G)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot the networks side by side\n",
       "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
       "\n",
       "for i, (name, G) in enumerate(networks.items()):\n",
       "    ax = axes[i]\n",
       "    pos = nx.spring_layout(G, seed=42)  # Position nodes using force-directed layout\n",
       "    nx.draw_networkx(G, pos=pos, ax=ax, node_size=40, with_labels=False, node_color='skyblue')\n",
       "    ax.set_title(f\"{name}\\nComplexity: {network_results[name]['complexity_index']:.4f}\")\n",
       "    ax.axis('off')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create a comparison table\n",
       "metrics_df = pd.DataFrame(network_results).T\n",
       "selected_metrics = ['avg_degree', 'clustering', 'centrality_std', 'num_communities', 'complexity_index']\n",
       "metrics_df[selected_metrics].round(4)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "This analysis shows that different network structures exhibit different types of complexity. Scale-free networks typically show high heterogeneity in node connections (high centrality_std), while small-world networks have high clustering coefficients. Our composite complexity index attempts to capture these different dimensions."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Dynamic Complexity Measures\n",
       "\n",
       "Dynamic complexity refers to the complexity of a system's behavior over time. Let's compare different time series using complexity measures."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def generate_time_series(type, n=1000, params=None):\n",
       "    \"\"\"Generate different types of time series.\"\"\"\n",
       "    np.random.seed(42)  # For reproducibility\n",
       "    \n",
       "    if type == 'random':\n",
       "        # White noise\n",
       "        return np.random.randn(n)\n",
       "    \n",
       "    elif type == 'sine':\n",
       "        # Simple sine wave\n",
       "        t = np.linspace(0, 10*np.pi, n)\n",
       "        return np.sin(t)\n",
       "    \n",
       "    elif type == 'composite':\n",
       "        # Multiple sine waves\n",
       "        t = np.linspace(0, 10*np.pi, n)\n",
       "        return np.sin(t) + 0.5*np.sin(2.5*t) + 0.25*np.sin(5*t)\n",
       "    \n",
       "    elif type == 'chaotic':\n",
       "        # Logistic map (chaotic regime)\n",
       "        r = 3.9  # Chaotic regime\n",
       "        x = np.zeros(n)\n",
       "        x[0] = 0.5  # Initial condition\n",
       "        for i in range(1, n):\n",
       "            x[i] = r * x[i-1] * (1 - x[i-1])\n",
       "        return x\n",
       "    \n",
       "    elif type == 'ar':\n",
       "        # Autoregressive process\n",
       "        ar_params = params if params else [0.7, -0.3]\n",
       "        return stats.arma_generate_sample(ar_params, [0], n)\n",
       "    \n",
       "    else:\n",
       "        raise ValueError(f\"Unknown time series type: {type}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def calculate_dynamic_complexity(time_series):\n",
       "    \"\"\"Calculate various dynamic complexity measures for a time series.\"\"\"\n",
       "    metrics = {}\n",
       "    \n",
       "    # Basic statistics\n",
       "    metrics['mean'] = np.mean(time_series)\n",
       "    metrics['std'] = np.std(time_series)\n",
       "    \n",
       "    # Entropy measures\n",
       "    # Convert to discrete bins for entropy calculation\n",
       "    bins = np.linspace(min(time_series), max(time_series), 30)\n",
       "    digitized = np.digitize(time_series, bins)\n",
       "    values, counts = np.unique(digitized, return_counts=True)\n",
       "    probabilities = counts / len(digitized)\n",
       "    metrics['entropy'] = -np.sum(probabilities * np.log2(probabilities))\n",
       "    \n",
       "    # Autocorrelation-based measures\n",
       "    autocorr = np.correlate(time_series - np.mean(time_series), \n",
       "                           time_series - np.mean(time_series), \n",
       "                           mode='full')\n",
       "    autocorr = autocorr[len(autocorr)//2:]\n",
       "    autocorr = autocorr / autocorr[0]  # Normalize\n",
       "    \n",
       "    # Decay rate of autocorrelation\n",
       "    metrics['autocorr_10'] = autocorr[10] if len(autocorr) > 10 else 0\n",
       "    \n",
       "    # Find first crossing of zero\n",
       "    zero_crossings = np.where(np.diff(np.signbit(autocorr)))[0]\n",
       "    metrics['decorrelation_time'] = zero_crossings[0] if len(zero_crossings) > 0 else len(autocorr)\n",
       "    \n",
       "    # Lyapunov Exponent approximation (simplified)\n",
       "    # Only meaningful for deterministic systems\n",
       "    n = len(time_series) - 1\n",
       "    epsilon = np.std(time_series) / 100\n",
       "    d0 = np.abs(np.diff(time_series))\n",
       "    metrics['lyapunov_approx'] = np.mean(np.log(np.maximum(d0, epsilon))) / np.log(2)\n",
       "    \n",
       "    # Approximate Kolmogorov complexity\n",
       "    # Using compression ratio as a proxy\n",
       "    import zlib\n",
       "    # Convert to bytes for compression\n",
       "    data_bytes = str(list(digitized)).encode()\n",
       "    compressed = zlib.compress(data_bytes)\n",
       "    metrics['kolmogorov_approx'] = len(compressed) / len(data_bytes)\n",
       "    \n",
       "    # Multi-scale entropy (simplified)\n",
       "    # We'll use sample entropy at different scales\n",
       "    scales = [1, 5, 10]\n",
       "    mse = []\n",
       "    for scale in scales:\n",
       "        # Coarse-graining process\n",
       "        coarse_grained = np.array([np.mean(time_series[i:i+scale]) \n",
       "                                 for i in range(0, len(time_series)-scale+1, scale)])\n",
       "        # Recalculate entropy\n",
       "        bins_cg = np.linspace(min(coarse_grained), max(coarse_grained), 20)\n",
       "        digitized_cg = np.digitize(coarse_grained, bins_cg)\n",
       "        values_cg, counts_cg = np.unique(digitized_cg, return_counts=True)\n",
       "        probabilities_cg = counts_cg / len(digitized_cg)\n",
       "        entropy_cg = -np.sum(probabilities_cg * np.log2(probabilities_cg + 1e-10))\n",
       "        mse.append(entropy_cg)\n",
       "    \n",
       "    metrics['mse_ratio'] = mse[-1] / mse[0] if mse[0] > 0 else 0\n",
       "    \n",
       "    return metrics"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Generate different time series\n",
       "time_series = {\n",
       "    'Random Noise': generate_time_series('random', n=500),\n",
       "    'Sine Wave': generate_time_series('sine', n=500),\n",
       "    'Composite Wave': generate_time_series('composite', n=500),\n",
       "    'Chaotic Map': generate_time_series('chaotic', n=500),\n",
       "    'AR Process': generate_time_series('ar', n=500)\n",
       "}\n",
       "\n",
       "# Calculate complexity metrics\n",
       "ts_results = {}\n",
       "for name, ts in time_series.items():\n",
       "    ts_results[name] = calculate_dynamic_complexity(ts)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot the time series\n",
       "fig, axes = plt.subplots(len(time_series), 1, figsize=(12, 10))\n",
       "\n",
       "for i, (name, ts) in enumerate(time_series.items()):\n",
       "    ax = axes[i]\n",
       "    ax.plot(ts)\n",
       "    ax.set_title(f\"{name} - Entropy: {ts_results[name]['entropy']:.2f}, \"\n",
       "                f\"K-Complexity: {ts_results[name]['kolmogorov_approx']:.2f}\")\n",
       "    ax.set_xlim(0, 200)  # Show just a portion for better visibility\n",
       "    ax.grid(True)\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create comparison table\n",
       "ts_df = pd.DataFrame(ts_results).T\n",
       "selected_ts_metrics = ['entropy', 'decorrelation_time', 'lyapunov_approx', 'kolmogorov_approx', 'mse_ratio']\n",
       "ts_df[selected_ts_metrics].round(4)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "The table shows how different metrics capture different aspects of dynamic complexity:\n",
       "\n",
       "- **Shannon Entropy**: Measures the unpredictability of the time series\n",
       "- **Decorrelation Time**: How quickly the system \"forgets\" its past\n",
       "- **Lyapunov Approximation**: Measures sensitivity to initial conditions (high for chaotic systems)\n",
       "- **Kolmogorov Complexity**: Algorithmic complexity (approximated via compression)\n",
       "- **Multi-scale Entropy Ratio**: How complexity changes across different time scales\n",
       "\n",
       "No single measure fully captures all aspects of complexity. The chaotic map shows interesting properties: high Lyapunov exponent but lower entropy than pure noise."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Computational Complexity\n",
       "\n",
       "Computational complexity focuses on the difficulty of simulating, predicting, or solving problems related to the system. Let's implement a simple example of computational complexity in cellular automata."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def run_elementary_ca(rule_number, size=100, steps=100, initial=None):\n",
       "    \"\"\"Run an elementary cellular automaton.\"\"\"\n",
       "    # Convert rule number to binary and pad to 8 bits\n",
       "    rule_binary = format(rule_number, '08b')\n",
       "    \n",
       "    # Initialize grid\n",
       "    grid = np.zeros((steps, size), dtype=int)\n",
       "    if initial is None:\n",
       "        # Start with a single cell in the middle\n",
       "        grid[0, size//2] = 1\n",
       "    else:\n",
       "        grid[0] = initial\n",
       "    \n",
       "    # Run CA\n",
       "    for i in range(1, steps):\n",
       "        for j in range(size):\n",
       "            # Get neighborhood with periodic boundary conditions\n",
       "            left = grid[i-1, (j-1) % size]\n",
       "            center = grid[i-1, j]\n",
       "            right = grid[i-1, (j+1) % size]\n",
       "            \n",
       "            # Convert neighborhood to binary pattern\n",
       "            pattern = (left << 2) | (center << 1) | right\n",
       "            \n",
       "            # Apply rule\n",
       "            grid[i, j] = int(rule_binary[7 - pattern])\n",
       "    \n",
       "    return grid"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def measure_ca_complexity(grid):\n",
       "    \"\"\"Measure computational complexity characteristics of CA evolution.\"\"\"\n",
       "    metrics = {}\n",
       "    \n",
       "    # Pattern complexity\n",
       "    # Use compression as approximation of Kolmogorov complexity\n",
       "    import zlib\n",
       "    for i in range(min(10, grid.shape[0])):\n",
       "        data_str = ''.join(map(str, grid[i]))\n",
       "        compressed = zlib.compress(data_str.encode())\n",
       "        metrics[f'row_{i}_compression_ratio'] = len(compressed) / len(data_str)\n",
       "    \n",
       "    # Growth of patterns\n",
       "    # Count non-zero cells in each row\n",
       "    active_cells = np.sum(grid, axis=1)\n",
       "    metrics['final_active_cells'] = active_cells[-1]\n",
       "    \n",
       "    # Growth rate (linear fit to active cells)\n",
       "    if np.any(active_cells):\n",
       "        x = np.arange(len(active_cells))\n",
       "        slope, _, _, _, _ = stats.linregress(x, active_cells)\n",
       "        metrics['growth_rate'] = slope\n",
       "    else:\n",
       "        metrics['growth_rate'] = 0\n",
       "        \n",
       "    # Entropy evolution\n",
       "    entropies = []\n",
       "    for i in range(grid.shape[0]):\n",
       "        values, counts = np.unique(grid[i], return_counts=True)\n",
       "        probabilities = counts / grid.shape[1]\n",
       "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
       "        entropies.append(entropy)\n",
       "    \n",
       "    metrics['avg_entropy'] = np.mean(entropies)\n",
       "    metrics['entropy_change'] = entropies[-1] - entropies[0] if entropies else 0\n",
       "    \n",
       "    # Final state entropy\n",
       "    metrics['final_entropy'] = entropies[-1] if entropies else 0\n",
       "    \n",
       "    return metrics"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Run different CA rules and measure their complexity\n",
       "ca_results = {}\n",
       "ca_grids = {}\n",
       "\n",
       "# Sample rules representing different Wolfram classes\n",
       "rules = {\n",
       "    'Class 1 (Rule 0)': 0,    # Evolution leads to homogeneous state\n",
       "    'Class 2 (Rule 4)': 4,    # Evolution leads to simple periodic patterns\n",
       "    'Class 3 (Rule 90)': 90,  # Evolution leads to chaotic patterns\n",
       "    'Class 4 (Rule 110)': 110  # Evolution leads to complex patterns (computational universality)\n",
       "}\n",
       "\n",
       "for name, rule in rules.items():\n",
       "    grid = run_elementary_ca(rule, size=100, steps=75)\n",
       "    ca_grids[name] = grid\n",
       "    ca_results[name] = measure_ca_complexity(grid)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot the CA evolutions\n",
       "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
       "axes = axes.flatten()\n",
       "\n",
       "for i, (name, grid) in enumerate(ca_grids.items()):\n",
       "    ax = axes[i]\n",
       "    ax.imshow(grid, cmap='binary', interpolation='none')\n",
       "    ax.set_title(f\"{name}\\nCompression Ratio: {ca_results[name]['row_9_compression_ratio']:.2f}, \"\n",
       "                f\"Entropy: {ca_results[name]['final_entropy']:.2f}\")\n",
       "    ax.set_xlabel('Cell position')\n",
       "    ax.set_ylabel('Time step')\n",
       "\n",
       "plt.tight_layout()\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create comparison table\n",
       "ca_df = pd.DataFrame(ca_results).T\n",
       "selected_ca_metrics = ['row_9_compression_ratio', 'avg_entropy', 'growth_rate', 'final_entropy']\n",
       "ca_df[selected_ca_metrics].round(4)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "This analysis shows how Wolfram's four classes of cellular automata exhibit different computational complexity:\n",
       "\n",
       "1. **Class 1**: Evolve to a homogeneous state, quickly becoming highly compressible with low entropy\n",
       "2. **Class 2**: Evolve to simple, stable or periodic structures - moderately compressible\n",
       "3. **Class 3**: Evolve to chaotic patterns with high entropy but relatively predictable statistics\n",
       "4. **Class 4**: Evolve to complex patterns with localized structures - these can support computation (Rule 110 is Turing complete!)\n",
       "\n",
       "Class 4 automata are particularly interesting because they sit at the \"edge of chaos\" - between order and disorder - where computational complexity is highest."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Integrated Example: Placing Systems in Complexity Space\n",
       "\n",
       "Now let's combine different metrics to place various systems in a \"complexity space\" to classify them:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Create synthetic data representing different systems\n",
       "np.random.seed(42)\n",
       "\n",
       "# Define metrics for our \"complexity space\"\n",
       "# 1. Number of components\n",
       "# 2. Interaction density\n",
       "# 3. Entropy\n",
       "# 4. Predictability (inverse)\n",
       "\n",
       "systems_data = {\n",
       "    'Pendulum': [3, 0.9, 0.1, 0.9],  # Few components, high interaction, low entropy, highly predictable\n",
       "    'Double Pendulum': [6, 0.8, 0.7, 0.3],  # More components, high interaction, higher entropy, less predictable\n",
       "    'Weather System': [100, 0.6, 0.8, 0.2],  # Many components, medium interaction, high entropy, low predictability\n",
       "    'Crystal': [1000, 0.1, 0.1, 0.9],  # Many components, low interaction, low entropy, highly predictable\n",
       "    'Gas': [1000, 0.1, 0.9, 0.4],  # Many components, low interaction, high entropy, moderate predictability\n",
       "    'Ecosystem': [500, 0.4, 0.7, 0.3],  # Many components, medium interaction, high entropy, low predictability\n",
       "    'Brain': [800, 0.7, 0.8, 0.2],  # Many components, high interaction, high entropy, low predictability\n",
       "    'Society': [700, 0.5, 0.8, 0.1],  # Many components, medium interaction, high entropy, very low predictability\n",
       "    'Computer': [500, 0.3, 0.3, 0.8],  # Many components, low-medium interaction, low entropy, high predictability\n",
       "    'Random Network': [50, 0.5, 0.9, 0.2]  # Medium components, medium interaction, high entropy, low predictability\n",
       "}\n",
       "\n",
       "# Add some random noise to make the visualization more interesting\n",
       "for system, values in systems_data.items():\n",
       "    systems_data[system] = [v + np.random.normal(0, 0.05) for v in values]\n",
       "    # Clip to valid range\n",
       "    systems_data[system] = [max(0, min(1, v)) for v in values]\n",
       "\n",
       "# Convert to DataFrame\n",
       "systems_df = pd.DataFrame(systems_data).T\n",
       "systems_df.columns = ['Num_Components', 'Interaction_Density', 'Entropy', 'Predictability']\n",
       "\n",
       "# Normalize number of components to 0-1 scale\n",
       "systems_df['Num_Components'] = systems_df['Num_Components'] / systems_df['Num_Components'].max()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Calculate composite complexity measures\n",
       "systems_df['Structural_Complexity'] = systems_df['Num_Components'] * systems_df['Interaction_Density']\n",
       "systems_df['Dynamic_Complexity'] = systems_df['Entropy'] * (1 - systems_df['Predictability'])\n",
       "systems_df['Total_Complexity'] = (systems_df['Structural_Complexity'] + systems_df['Dynamic_Complexity']) / 2\n",
       "\n",
       "# Sort by total complexity\n",
       "systems_df.sort_values('Total_Complexity', ascending=False)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Plot systems in the complexity space\n",
       "plt.figure(figsize=(10, 8))\n",
       "\n",
       "# Plot as scatter plot\n",
       "scatter = plt.scatter(systems_df['Structural_Complexity'], \n",
       "                     systems_df['Dynamic_Complexity'],\n",
       "                     c=systems_df['Total_Complexity'],\n",
       "                     s=150,\n",
       "                     alpha=0.7,\n",
       "                     cmap='viridis')\n",
       "\n",
       "# Add labels for each point\n",
       "for i, txt in enumerate(systems_df.index):\n",
       "    plt.annotate(txt, \n",
       "                (systems_df['Structural_Complexity'].iloc[i], systems_df['Dynamic_Complexity'].iloc[i]),\n",
       "                xytext=(5, 5), \n",
       "                textcoords='offset points')\n",
       "\n",
       "# Add colorbar\n",
       "cbar = plt.colorbar(scatter)\n",
       "cbar.set_label('Total Complexity')\n",
       "\n",
       "# Add dividing lines to create quadrants\n",
       "plt.axhline(0.4, color='gray', linestyle='--', alpha=0.5)\n",
       "plt.axvline(0.4, color='gray', linestyle='--', alpha=0.5)\n",
       "\n",
       "# Add quadrant labels\n",
       "plt.text(0.1, 0.1, \"Simple\", fontsize=12)\n",
       "plt.text(0.6, 0.1, \"Complicated\\n(Structural)\", fontsize=12)\n",
       "plt.text(0.1, 0.7, \"Chaotic\", fontsize=12)\n",
       "plt.text(0.6, 0.7, \"Complex\", fontsize=12)\n",
       "\n",
       "plt.xlabel('Structural Complexity')\n",
       "plt.ylabel('Dynamic Complexity')\n",
       "plt.title('Systems Mapped in Complexity Space')\n",
       "plt.grid(True, alpha=0.3)\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "This visualization places different systems in a complexity space defined by:\n",
       "\n",
       "- **Structural Complexity**: Combination of number of components and their interaction density\n",
       "- **Dynamic Complexity**: Combination of entropy and unpredictability\n",
       "\n",
       "The space divides into four quadrants representing different types of systems:\n",
       "\n",
       "1. **Simple**: Low structural and dynamic complexity (e.g., pendulum, crystal)\n",
       "2. **Complicated**: High structural but low dynamic complexity (e.g., computer)\n",
       "3. **Chaotic**: Low structural but high dynamic complexity (e.g., gas)\n",
       "4. **Complex**: High structural and dynamic complexity (e.g., brain, society, ecosystem)\n",
       "\n",
       "This demonstrates how we can build a taxonomy of complexity that allows us to classify and compare systems across different domains."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Your Turn: Classifying Real-World Systems\n",
       "\n",
       "Implement a function that takes textual descriptions of systems and attempts to place them in the complexity space based on key characteristics. You'll need to extract features related to structural and dynamic complexity."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def classify_system_complexity(description):\n",
       "    \"\"\"Analyze a system description and estimate its complexity metrics.\n",
       "    \n",
       "    Parameters:\n",
       "    - description: Text description of the system\n",
       "    \n",
       "    Returns:\n",
       "    - Dictionary with estimated complexity metrics\n",
       "    \"\"\"\n",
       "    # TODO: Implement a method to extract complexity metrics from descriptions\n",
       "    # You could use:  \n",
       "    # 1. Keyword counting (e.g., words like \"many\", \"interact\", \"predict\", \"random\")\n",
       "    # 2. Text analysis to estimate component count, interaction patterns, etc.\n",
       "    # 3. Classification based on similar systems in our database\n",
       "    \n",
       "    # Example implementation (placeholder):\n",
       "    metrics = {\n",
       "        'Num_Components': 0.5,  # Normalized 0-1\n",
       "        'Interaction_Density': 0.5,\n",
       "        'Entropy': 0.5,\n",
       "        'Predictability': 0.5\n",
       "    }\n",
       "    \n",
       "    # Calculate derived measures\n",
       "    metrics['Structural_Complexity'] = metrics['Num_Components'] * metrics['Interaction_Density']\n",
       "    metrics['Dynamic_Complexity'] = metrics['Entropy'] * (1 - metrics['Predictability'])\n",
       "    metrics['Total_Complexity'] = (metrics['Structural_Complexity'] + metrics['Dynamic_Complexity']) / 2\n",
       "    \n",
       "    return metrics"
      ]
     }
    ]
}
